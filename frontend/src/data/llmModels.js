export const mockModels = [
  {
    id: "1",
    name: "GPT-4o",
    provider: "OpenAI",
    context: "128K",
    capability: "Chat, Vision",
    tier: "Pro",
    category: "Text",
    price: "Pro",
    description: "Most capable GPT-4 model for complex reasoning and multimodal tasks.",
    fullDescription: "GPT-4o is OpenAI's most capable and balanced model, combining advanced reasoning with native multimodal capabilities. It excels at complex analysis, code generation, and understanding images, documents, and charts. Built for both high-stakes applications and everyday use with strong safety and reliability.",
    rating: 4.8,
    reviewCount: 1240,
    features: ["Complex reasoning & analysis", "Native vision (images, charts, docs)", "128K context window", "Code generation & explanation", "Multilingual support", "Function calling & tool use"],
    bestUseCases: ["Research and analysis", "Code review and generation", "Document understanding", "Creative writing", "Customer support automation"],
  },
  {
    id: "2",
    name: "Claude 3.5 Sonnet",
    provider: "Anthropic",
    context: "200K",
    capability: "Chat, Vision",
    tier: "Pro",
    category: "Text",
    price: "Pro",
    description: "Balanced speed and intelligence for coding and analysis.",
    fullDescription: "Claude 3.5 Sonnet offers an ideal balance of intelligence and speed. It is particularly strong at coding, analysis, and nuanced writing. With a 200K context window, it can process long documents and codebases while maintaining accuracy and coherence.",
    rating: 4.7,
    reviewCount: 892,
    features: ["200K context", "Strong coding & refactoring", "Vision and document analysis", "Balanced speed and quality", "Extended thinking", "Low latency"],
    bestUseCases: ["Software development", "Code review", "Long-form analysis", "Technical documentation", "Data extraction"],
  },
  {
    id: "3",
    name: "Gemini 1.5 Pro",
    provider: "Google",
    context: "1M",
    capability: "Chat, Vision",
    tier: "Pro",
    category: "Text",
    price: "Pro",
    description: "Massive context window for long documents and codebases.",
    fullDescription: "Gemini 1.5 Pro features a breakthrough 1 million token context window, enabling analysis of entire codebases, lengthy reports, and hours of video in a single request. It combines strong reasoning with native multimodal understanding.",
    rating: 4.6,
    reviewCount: 756,
    features: ["1M token context", "Video and audio understanding", "Codebase-level analysis", "Multimodal (text, image, video)", "Efficient long-context reasoning"],
    bestUseCases: ["Large codebase analysis", "Video summarization", "Legal and contract review", "Research synthesis", "Long document Q&A"],
  },
  {
    id: "4",
    name: "GPT-4o mini",
    provider: "OpenAI",
    context: "128K",
    capability: "Chat",
    tier: "Basic",
    category: "Text",
    price: "Basic",
    description: "Fast and affordable for everyday chat and simple tasks.",
    fullDescription: "GPT-4o mini is a fast, cost-effective model for everyday tasks. It delivers strong performance for chat, simple reasoning, and light coding while remaining affordable at scale for high-volume applications.",
    rating: 4.5,
    reviewCount: 2100,
    features: ["128K context", "Fast response times", "Lower cost", "Chat and completion", "Good for high volume"],
    bestUseCases: ["Customer chat", "Simple Q&A", "Drafting and editing", "Data formatting", "Light coding help"],
  },
  {
    id: "5",
    name: "Claude 3 Haiku",
    provider: "Anthropic",
    context: "200K",
    capability: "Chat",
    tier: "Basic",
    category: "Text",
    price: "Basic",
    description: "Lightweight and quick for high-volume workflows.",
    fullDescription: "Claude 3 Haiku is Anthropic's fastest and most compact model. It is built for high-throughput, low-latency workloads while still offering strong quality for summarization, extraction, and straightforward dialogue.",
    rating: 4.4,
    reviewCount: 1100,
    features: ["200K context", "Very low latency", "High throughput", "Summarization", "Structured extraction"],
    bestUseCases: ["Real-time chat", "Summarization", "Moderation", "Simple classification", "High-volume APIs"],
  },
  {
    id: "6",
    name: "Llama 3.1 70B",
    provider: "Meta",
    context: "128K",
    capability: "Completion",
    tier: "Pro",
    category: "Code",
    price: "Pro",
    description: "Open-weight model for completion and code generation.",
    fullDescription: "Llama 3.1 70B is a powerful open-weight model from Meta, suitable for code completion, general completion, and custom fine-tuning. It offers strong performance for developers who need control and flexibility.",
    rating: 4.3,
    reviewCount: 445,
    features: ["128K context", "Code completion", "Open weights", "Fine-tunable", "Self-hostable"],
    bestUseCases: ["Code completion", "Custom assistants", "On-premise deployment", "Fine-tuned workflows", "Cost-sensitive apps"],
  },
  {
    id: "7",
    name: "Mistral Large",
    provider: "Mistral",
    context: "128K",
    capability: "Chat, Function calling",
    tier: "Pro",
    category: "Business",
    price: "Pro",
    description: "Strong at function calling and structured outputs.",
    fullDescription: "Mistral Large is optimized for function calling and structured outputs, making it ideal for agents and tool-using applications. It combines strong reasoning with reliable JSON and schema adherence.",
    rating: 4.5,
    reviewCount: 320,
    features: ["128K context", "Function calling", "Structured outputs", "Multilingual", "Tool use"],
    bestUseCases: ["AI agents", "Structured data extraction", "Workflow automation", "API integrations", "Multi-step reasoning"],
  },
  {
    id: "8",
    name: "DALL-E 3",
    provider: "OpenAI",
    context: "—",
    capability: "Image",
    tier: "Pro",
    category: "Image",
    price: "Pro",
    description: "State-of-the-art image generation from text prompts.",
    fullDescription: "DALL-E 3 generates high-quality images from text descriptions with improved accuracy, composition, and detail. It supports various styles and aspect ratios and integrates with ChatGPT for iterative refinement.",
    rating: 4.7,
    reviewCount: 980,
    features: ["Text-to-image", "Multiple aspect ratios", "High detail", "Style control", "ChatGPT integration"],
    bestUseCases: ["Marketing visuals", "Concept art", "Product mockups", "Social content", "Illustration"],
  },
  {
    id: "9",
    name: "Stable Diffusion",
    provider: "Stability",
    context: "—",
    capability: "Image",
    tier: "Basic",
    category: "Image",
    price: "Basic",
    description: "Open-source image generation with fine-grained control.",
    fullDescription: "Stable Diffusion is an open-source image generation model that allows fine-grained control over style, composition, and output. It can be run locally or via API and is widely used for creative and commercial projects.",
    rating: 4.2,
    reviewCount: 1560,
    features: ["Open source", "Fine-grained control", "Local deployment", "Multiple checkpoints", "Img2img & inpainting"],
    bestUseCases: ["Art and design", "Prototyping", "Custom workflows", "Local/private generation", "Style experimentation"],
  },
  {
    id: "10",
    name: "Finance GPT",
    provider: "OpenAI",
    context: "128K",
    capability: "Chat",
    tier: "Pro",
    category: "Finance",
    price: "Enterprise",
    description: "Specialized for financial analysis and reporting.",
    fullDescription: "Finance GPT is tailored for financial professionals, supporting analysis, report generation, and compliance-oriented tasks. It is designed to handle financial terminology and structured data with high accuracy.",
    rating: 4.6,
    reviewCount: 210,
    features: ["Financial terminology", "Report generation", "128K context", "Structured analysis", "Compliance-aware"],
    bestUseCases: ["Financial reporting", "Market analysis", "Compliance drafts", "Investment research", "Risk summaries"],
  },
];

export function getModelById(id) {
  return mockModels.find((m) => m.id === id) ?? null;
}

export function getSimilarModels(model, limit = 4) {
  return mockModels
    .filter((m) => m.id !== model.id && (m.category === model.category || m.provider === model.provider))
    .slice(0, limit);
}
